{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "from os import path, getcwd\n",
    "import random\n",
    "from types import SimpleNamespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim, Tensor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///home/victor/ml_privacy_meter\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hInstalling collected packages: privacy-meter\r\n",
      "  Attempting uninstall: privacy-meter\r\n",
      "    Found existing installation: privacy-meter 1.0\r\n",
      "    Uninstalling privacy-meter-1.0:\r\n",
      "      Successfully uninstalled privacy-meter-1.0\r\n",
      "  Running setup.py develop for privacy-meter\r\n",
      "Successfully installed privacy-meter-1.0\r\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -e ../.\n",
    "\n",
    "from privacy_meter.audit import Audit\n",
    "from privacy_meter.audit_report import ROCCurveReport, SignalHistogramReport\n",
    "from privacy_meter.constants import InferenceGame\n",
    "from privacy_meter.dataset import Dataset\n",
    "from privacy_meter.hypothesis_test import threshold_func\n",
    "from privacy_meter.information_source import InformationSource\n",
    "from privacy_meter.information_source_signal import ModelLoss\n",
    "from privacy_meter.metric import ShadowMetric\n",
    "from privacy_meter.model import PytorchModel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x7f8e0c11caf0>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "N_TRAININGS = 5\n",
    "N_SHADOW_MODELS = 3\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def preprocess_purchase100():\n",
    "    \"\"\"\n",
    "    Cf. https://www.cs.cornell.edu/~shmat/shmat_oak17.pdf page 7\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    # Read raw dataset\n",
    "    dataset_path = \"../privacy_meter/dataset_purchase\"\n",
    "    with open(dataset_path, \"r\") as f:\n",
    "        purchase_dataset = f.readlines()\n",
    "    # Separate features and labels into different arrays\n",
    "    x, y = [], []\n",
    "    for datapoint in purchase_dataset:\n",
    "        split = datapoint.rstrip().split(\",\")\n",
    "        label = int(split[0]) - 1  # The first value is the label\n",
    "        features = np.array(split[1:], dtype=np.float32)  # The next values are the features\n",
    "        x.append(features)\n",
    "        y.append(label)\n",
    "    # Make sure the datatype is correct\n",
    "    x = np.array(x, dtype=np.float32)\n",
    "    # Convert labels into one hot vectors\n",
    "    y = OneHotEncoder(sparse=False).fit_transform(np.expand_dims(y, axis=1))\n",
    "    # Split data into train, test sets\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=1234)\n",
    "    return x_train, y_train, x_test, y_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def generate_splits(x_train, y_train, x_test, y_test, n_shadow_models):\n",
    "    dataset = Dataset(\n",
    "        data_dict={'train': {'x': x_train, 'y': y_train}, 'test': {'x': x_test, 'y': y_test}},\n",
    "        default_input='x',\n",
    "        default_output='y'\n",
    "    )\n",
    "    datasets_list = dataset.subdivide(\n",
    "        num_splits=n_shadow_models + 1,\n",
    "        return_results=True,\n",
    "        method='independent'\n",
    "    )\n",
    "    return datasets_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def generate_datasets(n_trainings, n_shadow_models):\n",
    "    x_train, y_train, x_test, y_test = preprocess_purchase100()\n",
    "    datasets_lists = [generate_splits(x_train, y_train, x_test, y_test, n_shadow_models) for _ in range(n_trainings)]\n",
    "    return datasets_lists"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def get_torch_models(n_trainings, n_shadow_models):\n",
    "    torch_models = [[\n",
    "        nn.Sequential(\n",
    "            nn.Linear(in_features=600, out_features=350),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=350, out_features=100),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        for _ in range(n_shadow_models + 1)\n",
    "        ] for _ in range(n_trainings)\n",
    "    ]\n",
    "    return torch_models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def get_trained_torch_models(n_trainings, n_shadow_models, criterion, datasets_lists, batch_size, epochs):\n",
    "    torch_models = get_torch_models(n_trainings, n_shadow_models)\n",
    "    for i in range(n_trainings):\n",
    "        for j in range(n_shadow_models + 1):\n",
    "            optimizer = optim.Adam(torch_models[i][j].parameters())\n",
    "            x = datasets_lists[i][j].get_feature(split_name=f'train', feature_name='<default_input>')\n",
    "            y = datasets_lists[i][j].get_feature(split_name=f'train', feature_name='<default_output>')\n",
    "            n_samples = x.shape[0]\n",
    "            n_batches = ceil(n_samples / batch_size)\n",
    "            x = np.array_split(x, n_batches)\n",
    "            y = np.array_split(y, n_batches)\n",
    "            for epoch in range(epochs):\n",
    "                epoch_loss, acc = 0.0, 0.0\n",
    "                for b in range(n_batches):\n",
    "                    optimizer.zero_grad()\n",
    "                    y_pred = torch_models[i][j](Tensor(x[b]))\n",
    "                    loss = criterion(Tensor(y[b]), y_pred)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    epoch_loss += loss.item()\n",
    "                    acc += torch.sum(y_pred.argmax(axis=1) == Tensor(y[b]).argmax(axis=1))\n",
    "                acc /= n_samples\n",
    "                epoch_loss /= n_samples\n",
    "                print(f'round #{i+1:02d}/{n_trainings:02d}, model #{j+1:02d}/{n_shadow_models+1:02d}, epoch #{epoch+1:02d}/{epochs:02d}:\\ttrain_acc = {acc:.3f}\\ttrain_loss = {epoch_loss:.3e}')\n",
    "    return torch_models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def get_models(n_trainings, n_shadow_models, datasets_lists, batch_size, epochs):\n",
    "    criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "    torch_models = get_trained_torch_models(n_trainings, n_shadow_models, criterion, datasets_lists, batch_size, epochs)\n",
    "    models = [[\n",
    "        PytorchModel(\n",
    "            model_obj=torch_models[i][j],\n",
    "            loss_fn=criterion\n",
    "        )\n",
    "        for j in range(n_shadow_models + 1)\n",
    "        ] for i in range(n_trainings)\n",
    "    ]\n",
    "    return models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def get_info_sources(models, datasets_lists, n_trainings):\n",
    "    target_info_sources = [InformationSource(\n",
    "        models=[models[i][0]],\n",
    "        datasets=[datasets_lists[i][0]]\n",
    "        ) for i in range(n_trainings)\n",
    "    ]\n",
    "    reference_info_sources = [InformationSource(\n",
    "        models=models[i][1:],\n",
    "        datasets=datasets_lists[i][1:]\n",
    "        ) for i in range(n_trainings)\n",
    "    ]\n",
    "    return target_info_sources, reference_info_sources"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def get_metrics(models, datasets_lists, n_trainings, empty=False):\n",
    "    if empty:\n",
    "        metrics = [ShadowMetric(\n",
    "            target_info_source=SimpleNamespace(models=[]),\n",
    "            reference_info_source=SimpleNamespace(models=[]),\n",
    "            signals=[ModelLoss()],\n",
    "            hypothesis_test_func=threshold_func,\n",
    "            unique_dataset=False,\n",
    "            reweight_samples=True\n",
    "        ) for _ in range(n_trainings)]\n",
    "    else:\n",
    "        target_info_sources, reference_info_sources = get_info_sources(models, datasets_lists, n_trainings)\n",
    "        metrics = [ShadowMetric(\n",
    "            target_info_source=target_info_sources[i],\n",
    "            reference_info_source=reference_info_sources[i],\n",
    "            signals=[ModelLoss()],\n",
    "            hypothesis_test_func=threshold_func,\n",
    "            unique_dataset=False,\n",
    "            reweight_samples=True\n",
    "        ) for i in range(n_trainings)]\n",
    "    return metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def get_audit_results(n_trainings, n_shadow_models, batch_size, epochs):\n",
    "    logs_directory_names = [path.join(getcwd(), f'test5_{i:02d}') for i in range(n_trainings)]\n",
    "    if all(path.isdir(d) for d in logs_directory_names):\n",
    "        metrics = get_metrics(None, None, n_trainings, True)\n",
    "    else:\n",
    "        datasets_lists = generate_datasets(n_trainings, n_shadow_models)\n",
    "        models = get_models(n_trainings, n_shadow_models, datasets_lists, batch_size, epochs)\n",
    "        metrics = get_metrics(models, datasets_lists, n_trainings)\n",
    "    audit = Audit(\n",
    "        metrics=metrics,\n",
    "        inference_game_type=InferenceGame.AVG_PRIVACY_LOSS_TRAINING_ALGO,\n",
    "        logs_directory_names=logs_directory_names\n",
    "    )\n",
    "    audit.prepare()\n",
    "    results = audit.run()\n",
    "    return results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = get_audit_results(N_TRAININGS, N_SHADOW_MODELS, BATCH_SIZE, EPOCHS)\n",
    "for result in results:\n",
    "    print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This instruction won't be needed once the tool is on pip\n",
    "from privacy_meter import audit_report\n",
    "audit_report.REPORT_FILES_DIR = '../privacy_meter/report_files'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ROCCurveReport.generate_report(\n",
    "    metric_result=results,\n",
    "    inference_game_type=InferenceGame.AVG_PRIVACY_LOSS_TRAINING_ALGO,\n",
    "    save=False,\n",
    "    show=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SignalHistogramReport.generate_report(\n",
    "    metric_result=results,\n",
    "    inference_game_type=InferenceGame.AVG_PRIVACY_LOSS_TRAINING_ALGO,\n",
    "    save=False,\n",
    "    show=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}