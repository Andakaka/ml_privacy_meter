{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Shadow models audit for a PyTorch model trained on Purchase100"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this tutorial, we will see:\n",
    "* How to create a Dataset object manually\n",
    "* How to audit a PyTorch model\n",
    "* How to use the ShadowMetric"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch import nn, optim, Tensor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///home/victor/ml_privacy_meter\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25hInstalling collected packages: privacy-meter\n",
      "  Attempting uninstall: privacy-meter\n",
      "    Found existing installation: privacy-meter 1.0\n",
      "    Uninstalling privacy-meter-1.0:\n",
      "      Successfully uninstalled privacy-meter-1.0\n",
      "  Running setup.py develop for privacy-meter\n",
      "Successfully installed privacy-meter-1.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -e ../.\n",
    "from privacy_meter.audit import Audit\n",
    "from privacy_meter.dataset import Dataset\n",
    "from privacy_meter.hypothesis_test import threshold_func\n",
    "from privacy_meter.information_source import InformationSource\n",
    "from privacy_meter.information_source_signal import ModelLoss\n",
    "from privacy_meter.metric import ShadowMetric\n",
    "from privacy_meter.model import PytorchModel"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Settings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_shadow_models = 5\n",
    "epochs = 20\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset creation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's download the Purchase100 dataset (presented in https://www.cs.cornell.edu/~shmat/shmat_oak17.pdf on page 7) and extract it:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-03-26 15:23:30--  https://github.com/privacytrustlab/datasets/raw/master/dataset_purchase.tgz\n",
      "Resolving github.com (github.com)... 20.205.243.166\n",
      "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/privacytrustlab/datasets/master/dataset_purchase.tgz [following]\n",
      "--2022-03-26 15:23:30--  https://raw.githubusercontent.com/privacytrustlab/datasets/master/dataset_purchase.tgz\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 22045876 (21M) [application/octet-stream]\n",
      "Saving to: ‘dataset_purchase.tgz’\n",
      "\n",
      "dataset_purchase.tg 100%[===================>]  21.02M  10.6MB/s    in 2.0s    \n",
      "\n",
      "2022-03-26 15:23:32 (10.6 MB/s) - ‘dataset_purchase.tgz’ saved [22045876/22045876]\n",
      "\n",
      "dataset_purchase\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/privacytrustlab/datasets/raw/master/dataset_purchase.tgz\n",
    "!tar -xvzf dataset_purchase.tgz\n",
    "!rm dataset_purchase.tgz"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We need to read the file and preprocess the dataset:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_purchase100():\n",
    "    \"\"\"\n",
    "    Cf. https://www.cs.cornell.edu/~shmat/shmat_oak17.pdf page 7\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    # Read raw dataset\n",
    "    dataset_path = \"dataset_purchase\"\n",
    "    with open(dataset_path, \"r\") as f:\n",
    "        purchase_dataset = f.readlines()\n",
    "    # Separate features and labels into different arrays\n",
    "    x, y = [], []\n",
    "    for datapoint in purchase_dataset:\n",
    "        split = datapoint.rstrip().split(\",\")\n",
    "        label = int(split[0]) - 1  # The first value is the label\n",
    "        features = np.array(split[1:], dtype=np.float32)  # The next values are the features\n",
    "        x.append(features)\n",
    "        y.append(label)\n",
    "    # Make sure the datatype is correct\n",
    "    x = np.array(x, dtype=np.float32)\n",
    "    # Convert labels into one hot vectors\n",
    "    y = OneHotEncoder(sparse=False).fit_transform(np.expand_dims(y, axis=1))\n",
    "    # Split data into train, test sets\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=1234)\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = preprocess_purchase100()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, we wrap the dataset into a Dataset object:\n",
    "* `data_dict` contains the actual dataset, in the form of a 2D dictionary. The first key corresponds to the split name (here we have two: \"train\" and \"test\"), and the second key to the feature name (here we also have two: \"x\" and \"y\").\n",
    "* `default_input` contains the name of the feature that should be used as the models input (here \"x\").\n",
    "* `default_output` contains the name of the feature that should be used as the label / models output (here \"y\")."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = Dataset(\n",
    "    data_dict={'train': {'x': x_train, 'y': y_train}, 'test': {'x': x_test, 'y': y_test}},\n",
    "    default_input='x',\n",
    "    default_output='y'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we use the built-in `Dataset.subdivide()` function, to split the two splits (\"train\" and \"test\") into multiple sub-splits (one per model). Their names will be \"train000\", \"train001\", etc. and \"test000\", \"test001\", etc."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset.subdivide(num_splits=n_shadow_models + 1, delete_original=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's now define the pytorch models to be used: one target model, and `n_shadow_models` shadow models."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch_models = [\n",
    "    nn.Sequential(\n",
    "        nn.Linear(in_features=600, out_features=128),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(in_features=128, out_features=100),\n",
    "        nn.Softmax()\n",
    "    )\n",
    "    for _ in range(n_shadow_models + 1)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We define the loss:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "And we train each model on its split of the dataset:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victor/ml_privacy_meter/venv/lib/python3.8/site-packages/torch/nn/modules/container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model #00, epoch #000:\ttrain_acc = 0.229\ttrain_loss = 4.418e+00\n",
      "model #00, epoch #001:\ttrain_acc = 0.401\ttrain_loss = 4.252e+00\n",
      "model #00, epoch #002:\ttrain_acc = 0.470\ttrain_loss = 4.178e+00\n",
      "model #00, epoch #003:\ttrain_acc = 0.512\ttrain_loss = 4.131e+00\n",
      "model #00, epoch #004:\ttrain_acc = 0.542\ttrain_loss = 4.097e+00\n",
      "model #00, epoch #005:\ttrain_acc = 0.569\ttrain_loss = 4.070e+00\n",
      "model #00, epoch #006:\ttrain_acc = 0.582\ttrain_loss = 4.052e+00\n",
      "model #00, epoch #007:\ttrain_acc = 0.595\ttrain_loss = 4.039e+00\n",
      "model #00, epoch #008:\ttrain_acc = 0.603\ttrain_loss = 4.028e+00\n",
      "model #00, epoch #009:\ttrain_acc = 0.613\ttrain_loss = 4.018e+00\n",
      "model #00, epoch #010:\ttrain_acc = 0.629\ttrain_loss = 4.003e+00\n",
      "model #00, epoch #011:\ttrain_acc = 0.637\ttrain_loss = 3.993e+00\n",
      "model #00, epoch #012:\ttrain_acc = 0.639\ttrain_loss = 3.990e+00\n",
      "model #00, epoch #013:\ttrain_acc = 0.640\ttrain_loss = 3.987e+00\n",
      "model #00, epoch #014:\ttrain_acc = 0.641\ttrain_loss = 3.986e+00\n",
      "model #00, epoch #015:\ttrain_acc = 0.642\ttrain_loss = 3.985e+00\n",
      "model #00, epoch #016:\ttrain_acc = 0.642\ttrain_loss = 3.984e+00\n",
      "model #00, epoch #017:\ttrain_acc = 0.643\ttrain_loss = 3.983e+00\n",
      "model #00, epoch #018:\ttrain_acc = 0.642\ttrain_loss = 3.983e+00\n",
      "model #00, epoch #019:\ttrain_acc = 0.640\ttrain_loss = 3.986e+00\n",
      "model #01, epoch #000:\ttrain_acc = 0.212\ttrain_loss = 4.431e+00\n",
      "model #01, epoch #001:\ttrain_acc = 0.359\ttrain_loss = 4.284e+00\n",
      "model #01, epoch #002:\ttrain_acc = 0.413\ttrain_loss = 4.227e+00\n",
      "model #01, epoch #003:\ttrain_acc = 0.438\ttrain_loss = 4.198e+00\n",
      "model #01, epoch #004:\ttrain_acc = 0.483\ttrain_loss = 4.156e+00\n",
      "model #01, epoch #005:\ttrain_acc = 0.526\ttrain_loss = 4.114e+00\n",
      "model #01, epoch #006:\ttrain_acc = 0.565\ttrain_loss = 4.074e+00\n",
      "model #01, epoch #007:\ttrain_acc = 0.590\ttrain_loss = 4.048e+00\n",
      "model #01, epoch #008:\ttrain_acc = 0.603\ttrain_loss = 4.032e+00\n",
      "model #01, epoch #009:\ttrain_acc = 0.614\ttrain_loss = 4.020e+00\n",
      "model #01, epoch #010:\ttrain_acc = 0.620\ttrain_loss = 4.011e+00\n",
      "model #01, epoch #011:\ttrain_acc = 0.622\ttrain_loss = 4.007e+00\n",
      "model #01, epoch #012:\ttrain_acc = 0.623\ttrain_loss = 4.006e+00\n",
      "model #01, epoch #013:\ttrain_acc = 0.624\ttrain_loss = 4.004e+00\n",
      "model #01, epoch #014:\ttrain_acc = 0.624\ttrain_loss = 4.003e+00\n",
      "model #01, epoch #015:\ttrain_acc = 0.627\ttrain_loss = 4.000e+00\n",
      "model #01, epoch #016:\ttrain_acc = 0.629\ttrain_loss = 3.997e+00\n",
      "model #01, epoch #017:\ttrain_acc = 0.631\ttrain_loss = 3.994e+00\n",
      "model #01, epoch #018:\ttrain_acc = 0.630\ttrain_loss = 3.996e+00\n",
      "model #01, epoch #019:\ttrain_acc = 0.632\ttrain_loss = 3.994e+00\n",
      "model #02, epoch #000:\ttrain_acc = 0.228\ttrain_loss = 4.421e+00\n",
      "model #02, epoch #001:\ttrain_acc = 0.380\ttrain_loss = 4.268e+00\n",
      "model #02, epoch #002:\ttrain_acc = 0.421\ttrain_loss = 4.219e+00\n",
      "model #02, epoch #003:\ttrain_acc = 0.453\ttrain_loss = 4.184e+00\n",
      "model #02, epoch #004:\ttrain_acc = 0.498\ttrain_loss = 4.141e+00\n",
      "model #02, epoch #005:\ttrain_acc = 0.543\ttrain_loss = 4.097e+00\n",
      "model #02, epoch #006:\ttrain_acc = 0.583\ttrain_loss = 4.057e+00\n",
      "model #02, epoch #007:\ttrain_acc = 0.625\ttrain_loss = 4.016e+00\n",
      "model #02, epoch #008:\ttrain_acc = 0.663\ttrain_loss = 3.977e+00\n",
      "model #02, epoch #009:\ttrain_acc = 0.679\ttrain_loss = 3.958e+00\n",
      "model #02, epoch #010:\ttrain_acc = 0.700\ttrain_loss = 3.938e+00\n",
      "model #02, epoch #011:\ttrain_acc = 0.708\ttrain_loss = 3.925e+00\n",
      "model #02, epoch #012:\ttrain_acc = 0.712\ttrain_loss = 3.919e+00\n",
      "model #02, epoch #013:\ttrain_acc = 0.713\ttrain_loss = 3.917e+00\n",
      "model #02, epoch #014:\ttrain_acc = 0.714\ttrain_loss = 3.915e+00\n",
      "model #02, epoch #015:\ttrain_acc = 0.715\ttrain_loss = 3.914e+00\n",
      "model #02, epoch #016:\ttrain_acc = 0.715\ttrain_loss = 3.912e+00\n",
      "model #02, epoch #017:\ttrain_acc = 0.715\ttrain_loss = 3.912e+00\n",
      "model #02, epoch #018:\ttrain_acc = 0.716\ttrain_loss = 3.911e+00\n",
      "model #02, epoch #019:\ttrain_acc = 0.716\ttrain_loss = 3.911e+00\n",
      "model #03, epoch #000:\ttrain_acc = 0.230\ttrain_loss = 4.417e+00\n",
      "model #03, epoch #001:\ttrain_acc = 0.402\ttrain_loss = 4.250e+00\n",
      "model #03, epoch #002:\ttrain_acc = 0.456\ttrain_loss = 4.188e+00\n",
      "model #03, epoch #003:\ttrain_acc = 0.477\ttrain_loss = 4.161e+00\n",
      "model #03, epoch #004:\ttrain_acc = 0.513\ttrain_loss = 4.126e+00\n",
      "model #03, epoch #005:\ttrain_acc = 0.526\ttrain_loss = 4.108e+00\n",
      "model #03, epoch #006:\ttrain_acc = 0.544\ttrain_loss = 4.090e+00\n",
      "model #03, epoch #007:\ttrain_acc = 0.569\ttrain_loss = 4.064e+00\n",
      "model #03, epoch #008:\ttrain_acc = 0.578\ttrain_loss = 4.054e+00\n",
      "model #03, epoch #009:\ttrain_acc = 0.597\ttrain_loss = 4.036e+00\n",
      "model #03, epoch #010:\ttrain_acc = 0.608\ttrain_loss = 4.023e+00\n",
      "model #03, epoch #011:\ttrain_acc = 0.612\ttrain_loss = 4.017e+00\n",
      "model #03, epoch #012:\ttrain_acc = 0.614\ttrain_loss = 4.014e+00\n",
      "model #03, epoch #013:\ttrain_acc = 0.615\ttrain_loss = 4.012e+00\n",
      "model #03, epoch #014:\ttrain_acc = 0.616\ttrain_loss = 4.011e+00\n",
      "model #03, epoch #015:\ttrain_acc = 0.617\ttrain_loss = 4.009e+00\n",
      "model #03, epoch #016:\ttrain_acc = 0.617\ttrain_loss = 4.010e+00\n",
      "model #03, epoch #017:\ttrain_acc = 0.617\ttrain_loss = 4.009e+00\n",
      "model #03, epoch #018:\ttrain_acc = 0.618\ttrain_loss = 4.008e+00\n",
      "model #03, epoch #019:\ttrain_acc = 0.618\ttrain_loss = 4.007e+00\n",
      "model #04, epoch #000:\ttrain_acc = 0.229\ttrain_loss = 4.418e+00\n",
      "model #04, epoch #001:\ttrain_acc = 0.393\ttrain_loss = 4.256e+00\n",
      "model #04, epoch #002:\ttrain_acc = 0.447\ttrain_loss = 4.197e+00\n",
      "model #04, epoch #003:\ttrain_acc = 0.468\ttrain_loss = 4.169e+00\n",
      "model #04, epoch #004:\ttrain_acc = 0.480\ttrain_loss = 4.153e+00\n",
      "model #04, epoch #005:\ttrain_acc = 0.506\ttrain_loss = 4.128e+00\n",
      "model #04, epoch #006:\ttrain_acc = 0.534\ttrain_loss = 4.101e+00\n",
      "model #04, epoch #007:\ttrain_acc = 0.571\ttrain_loss = 4.065e+00\n",
      "model #04, epoch #008:\ttrain_acc = 0.589\ttrain_loss = 4.044e+00\n",
      "model #04, epoch #009:\ttrain_acc = 0.597\ttrain_loss = 4.035e+00\n",
      "model #04, epoch #010:\ttrain_acc = 0.598\ttrain_loss = 4.031e+00\n",
      "model #04, epoch #011:\ttrain_acc = 0.600\ttrain_loss = 4.028e+00\n",
      "model #04, epoch #012:\ttrain_acc = 0.600\ttrain_loss = 4.027e+00\n",
      "model #04, epoch #013:\ttrain_acc = 0.601\ttrain_loss = 4.025e+00\n",
      "model #04, epoch #014:\ttrain_acc = 0.602\ttrain_loss = 4.024e+00\n",
      "model #04, epoch #015:\ttrain_acc = 0.602\ttrain_loss = 4.024e+00\n",
      "model #04, epoch #016:\ttrain_acc = 0.608\ttrain_loss = 4.019e+00\n",
      "model #04, epoch #017:\ttrain_acc = 0.611\ttrain_loss = 4.016e+00\n",
      "model #04, epoch #018:\ttrain_acc = 0.614\ttrain_loss = 4.012e+00\n",
      "model #04, epoch #019:\ttrain_acc = 0.616\ttrain_loss = 4.011e+00\n",
      "model #05, epoch #000:\ttrain_acc = 0.227\ttrain_loss = 4.423e+00\n",
      "model #05, epoch #001:\ttrain_acc = 0.368\ttrain_loss = 4.278e+00\n",
      "model #05, epoch #002:\ttrain_acc = 0.414\ttrain_loss = 4.227e+00\n",
      "model #05, epoch #003:\ttrain_acc = 0.481\ttrain_loss = 4.163e+00\n",
      "model #05, epoch #004:\ttrain_acc = 0.525\ttrain_loss = 4.117e+00\n",
      "model #05, epoch #005:\ttrain_acc = 0.549\ttrain_loss = 4.089e+00\n",
      "model #05, epoch #006:\ttrain_acc = 0.583\ttrain_loss = 4.056e+00\n",
      "model #05, epoch #007:\ttrain_acc = 0.617\ttrain_loss = 4.020e+00\n",
      "model #05, epoch #008:\ttrain_acc = 0.630\ttrain_loss = 4.005e+00\n",
      "model #05, epoch #009:\ttrain_acc = 0.635\ttrain_loss = 3.996e+00\n",
      "model #05, epoch #010:\ttrain_acc = 0.638\ttrain_loss = 3.992e+00\n",
      "model #05, epoch #011:\ttrain_acc = 0.640\ttrain_loss = 3.989e+00\n",
      "model #05, epoch #012:\ttrain_acc = 0.641\ttrain_loss = 3.987e+00\n",
      "model #05, epoch #013:\ttrain_acc = 0.642\ttrain_loss = 3.985e+00\n",
      "model #05, epoch #014:\ttrain_acc = 0.641\ttrain_loss = 3.985e+00\n",
      "model #05, epoch #015:\ttrain_acc = 0.642\ttrain_loss = 3.983e+00\n",
      "model #05, epoch #016:\ttrain_acc = 0.642\ttrain_loss = 3.983e+00\n",
      "model #05, epoch #017:\ttrain_acc = 0.642\ttrain_loss = 3.983e+00\n",
      "model #05, epoch #018:\ttrain_acc = 0.642\ttrain_loss = 3.983e+00\n",
      "model #05, epoch #019:\ttrain_acc = 0.643\ttrain_loss = 3.982e+00\n"
     ]
    }
   ],
   "source": [
    "for k, model in enumerate(torch_models):\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    x = dataset.get_feature(split_name=f'train{k:03d}', feature_name='<default_input>')\n",
    "    y = dataset.get_feature(split_name=f'train{k:03d}', feature_name='<default_output>')\n",
    "    n_samples = x.shape[0]\n",
    "    n_batches = ceil(n_samples / batch_size)\n",
    "    x = np.array_split(x, n_batches)\n",
    "    y = np.array_split(y, n_batches)\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss, acc = 0.0, 0.0\n",
    "        for b in range(n_batches):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(Tensor(x[b]))\n",
    "            loss = criterion(Tensor(y[b]), y_pred)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            acc += torch.sum(y_pred.argmax(axis=1) == Tensor(y[b]).argmax(axis=1))\n",
    "        acc /= n_samples\n",
    "        epoch_loss /= n_samples\n",
    "        print(f'model #{k:02d}, epoch #{epoch:03d}:\\ttrain_acc = {acc:.3f}\\ttrain_loss = {epoch_loss:.3e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that the models are all train, we can wrap each of them in a `PytorchModel` object:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "models = [\n",
    "    PytorchModel(\n",
    "        model_obj=model,\n",
    "        loss_fn=criterion\n",
    "    )\n",
    "    for model in torch_models\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Information Sources"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now define two InformationSource object. Basically, an information source is an abstraction representing a set of models, and their corresponding dataset. Note that we use the same `dataset` variable for the two InformationSource objects, but each will be queried on different splits of the dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "target_info_source = InformationSource(\n",
    "    models=[models[0]],\n",
    "    datasets=[dataset]\n",
    ")\n",
    "\n",
    "reference_info_source = InformationSource(\n",
    "    models=models[1:],\n",
    "    datasets=[dataset]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Metric and Audit"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "metric = ShadowMetric(\n",
    "    target_info_source=target_info_source,\n",
    "    reference_info_source=reference_info_source,\n",
    "    signals=[ModelLoss()],\n",
    "    hypothesis_test_func=threshold_func,\n",
    "    unique_dataset=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= METRIC RESULT OBJECT =============\n",
      "Accuracy          = 0.5\n",
      "ROC AUC Score     = 0.5\n",
      "FPR               = 1.0\n",
      "TN, FP, FN, TP    = (0, 1, 0, 1)\n"
     ]
    }
   ],
   "source": [
    "audit = Audit(\n",
    "    metric=metric,\n",
    "    target_info_source=target_info_source,\n",
    "    reference_info_source=reference_info_source\n",
    ")\n",
    "\n",
    "print(audit.run())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}