{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Shadow models audit for a PyTorch model trained on Purchase100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this tutorial, we will see:\n",
    "* How to create a Dataset object manually\n",
    "* How to audit a PyTorch model\n",
    "* How to use the ShadowMetric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch import nn, optim, Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For now we install the library from the local source. A version will be pushed to pip soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///home/victor/ml_privacy_meter\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25hInstalling collected packages: privacy-meter\n",
      "  Attempting uninstall: privacy-meter\n",
      "    Found existing installation: privacy-meter 1.0\n",
      "    Uninstalling privacy-meter-1.0:\n",
      "      Successfully uninstalled privacy-meter-1.0\n",
      "  Running setup.py develop for privacy-meter\n",
      "Successfully installed privacy-meter-1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-28 11:44:49.984452: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-03-28 11:44:49.984478: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -e ../.\n",
    "from privacy_meter.audit import Audit\n",
    "from privacy_meter.dataset import Dataset\n",
    "from privacy_meter.hypothesis_test import threshold_func\n",
    "from privacy_meter.information_source import InformationSource\n",
    "from privacy_meter.information_source_signal import ModelLoss\n",
    "from privacy_meter.metric import ShadowMetric\n",
    "from privacy_meter.model import PytorchModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Setting seed for reproducibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Hyper parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_shadow_models = 3  # Number of shadow models to be trained\n",
    "epochs = 10          # Number of epochs for each model\n",
    "batch_size = 8       # Batch size for the trainings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dataset creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's download the Purchase100 dataset (presented in https://www.cs.cornell.edu/~shmat/shmat_oak17.pdf on page 7) and extract it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-03-28 11:44:52--  https://github.com/privacytrustlab/datasets/raw/master/dataset_purchase.tgz\n",
      "Resolving github.com (github.com)... 20.205.243.166\n",
      "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/privacytrustlab/datasets/master/dataset_purchase.tgz [following]\n",
      "--2022-03-28 11:44:52--  https://raw.githubusercontent.com/privacytrustlab/datasets/master/dataset_purchase.tgz\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 22045876 (21M) [application/octet-stream]\n",
      "Saving to: ‘dataset_purchase.tgz’\n",
      "\n",
      "dataset_purchase.tg 100%[===================>]  21.02M  11.1MB/s    in 1.9s    \n",
      "\n",
      "2022-03-28 11:44:58 (11.1 MB/s) - ‘dataset_purchase.tgz’ saved [22045876/22045876]\n",
      "\n",
      "dataset_purchase\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/privacytrustlab/datasets/raw/master/dataset_purchase.tgz\n",
    "!tar -xvzf dataset_purchase.tgz\n",
    "!rm dataset_purchase.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We need to read the file and preprocess the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_purchase100():\n",
    "    \"\"\"\n",
    "    Cf. https://www.cs.cornell.edu/~shmat/shmat_oak17.pdf page 7\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    # Read raw dataset\n",
    "    dataset_path = \"dataset_purchase\"\n",
    "    with open(dataset_path, \"r\") as f:\n",
    "        purchase_dataset = f.readlines()\n",
    "    # Separate features and labels into different arrays\n",
    "    x, y = [], []\n",
    "    for datapoint in purchase_dataset:\n",
    "        split = datapoint.rstrip().split(\",\")\n",
    "        label = int(split[0]) - 1  # The first value is the label\n",
    "        features = np.array(split[1:], dtype=np.float32)  # The next values are the features\n",
    "        x.append(features)\n",
    "        y.append(label)\n",
    "    # Make sure the datatype is correct\n",
    "    x = np.array(x, dtype=np.float32)\n",
    "    # Convert labels into one hot vectors\n",
    "    y = OneHotEncoder(sparse=False).fit_transform(np.expand_dims(y, axis=1))\n",
    "    # Split data into train, test sets\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=1234)\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = preprocess_purchase100()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Then, we wrap the dataset into a Dataset object:\n",
    "* `data_dict` contains the actual dataset, in the form of a 2D dictionary. The first key corresponds to the split name (here we have two: \"train\" and \"test\"), and the second key to the feature name (here we also have two: \"x\" and \"y\").\n",
    "* `default_input` contains the name of the feature that should be used as the models input (here \"x\").\n",
    "* `default_output` contains the name of the feature that should be used as the label / models output (here \"y\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = Dataset(\n",
    "    data_dict={'train': {'x': x_train, 'y': y_train}, 'test': {'x': x_test, 'y': y_test}},\n",
    "    default_input='x',\n",
    "    default_output='y'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Finally, we use the built-in `Dataset.subdivide()` function, to split the two splits (\"train\" and \"test\") into multiple sub-splits (one per model). Their names will be \"train000\", \"train001\", etc. and \"test000\", \"test001\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset.subdivide(num_splits=n_shadow_models + 1, delete_original=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's now define the pytorch models to be used: one target model, and `n_shadow_models` shadow models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch_models = [\n",
    "    nn.Sequential(\n",
    "        nn.Linear(in_features=600, out_features=128),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(in_features=128, out_features=100),\n",
    "        nn.Softmax(dim=1)\n",
    "    )\n",
    "    for _ in range(n_shadow_models + 1)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We define the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "And we train each model on its split of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victor/ml_privacy_meter/venv/lib/python3.8/site-packages/torch/nn/modules/container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model #00, epoch #000:\ttrain_acc = 0.252\ttrain_loss = 4.394e+00\n",
      "model #00, epoch #001:\ttrain_acc = 0.394\ttrain_loss = 4.250e+00\n",
      "model #00, epoch #002:\ttrain_acc = 0.455\ttrain_loss = 4.188e+00\n",
      "model #00, epoch #003:\ttrain_acc = 0.512\ttrain_loss = 4.131e+00\n",
      "model #00, epoch #004:\ttrain_acc = 0.537\ttrain_loss = 4.102e+00\n",
      "model #00, epoch #005:\ttrain_acc = 0.559\ttrain_loss = 4.078e+00\n",
      "model #00, epoch #006:\ttrain_acc = 0.580\ttrain_loss = 4.056e+00\n",
      "model #00, epoch #007:\ttrain_acc = 0.612\ttrain_loss = 4.025e+00\n",
      "model #00, epoch #008:\ttrain_acc = 0.626\ttrain_loss = 4.008e+00\n",
      "model #00, epoch #009:\ttrain_acc = 0.633\ttrain_loss = 3.999e+00\n",
      "model #01, epoch #000:\ttrain_acc = 0.271\ttrain_loss = 4.377e+00\n",
      "model #01, epoch #001:\ttrain_acc = 0.411\ttrain_loss = 4.235e+00\n",
      "model #01, epoch #002:\ttrain_acc = 0.460\ttrain_loss = 4.182e+00\n",
      "model #01, epoch #003:\ttrain_acc = 0.498\ttrain_loss = 4.141e+00\n",
      "model #01, epoch #004:\ttrain_acc = 0.529\ttrain_loss = 4.109e+00\n",
      "model #01, epoch #005:\ttrain_acc = 0.573\ttrain_loss = 4.066e+00\n",
      "model #01, epoch #006:\ttrain_acc = 0.590\ttrain_loss = 4.046e+00\n",
      "model #01, epoch #007:\ttrain_acc = 0.597\ttrain_loss = 4.036e+00\n",
      "model #01, epoch #008:\ttrain_acc = 0.602\ttrain_loss = 4.029e+00\n",
      "model #01, epoch #009:\ttrain_acc = 0.607\ttrain_loss = 4.023e+00\n",
      "model #02, epoch #000:\ttrain_acc = 0.261\ttrain_loss = 4.387e+00\n",
      "model #02, epoch #001:\ttrain_acc = 0.437\ttrain_loss = 4.213e+00\n",
      "model #02, epoch #002:\ttrain_acc = 0.476\ttrain_loss = 4.166e+00\n",
      "model #02, epoch #003:\ttrain_acc = 0.524\ttrain_loss = 4.117e+00\n",
      "model #02, epoch #004:\ttrain_acc = 0.554\ttrain_loss = 4.086e+00\n",
      "model #02, epoch #005:\ttrain_acc = 0.584\ttrain_loss = 4.056e+00\n",
      "model #02, epoch #006:\ttrain_acc = 0.601\ttrain_loss = 4.035e+00\n",
      "model #02, epoch #007:\ttrain_acc = 0.609\ttrain_loss = 4.025e+00\n",
      "model #02, epoch #008:\ttrain_acc = 0.613\ttrain_loss = 4.019e+00\n",
      "model #02, epoch #009:\ttrain_acc = 0.614\ttrain_loss = 4.016e+00\n",
      "model #03, epoch #000:\ttrain_acc = 0.268\ttrain_loss = 4.380e+00\n",
      "model #03, epoch #001:\ttrain_acc = 0.395\ttrain_loss = 4.248e+00\n",
      "model #03, epoch #002:\ttrain_acc = 0.439\ttrain_loss = 4.199e+00\n",
      "model #03, epoch #003:\ttrain_acc = 0.475\ttrain_loss = 4.163e+00\n",
      "model #03, epoch #004:\ttrain_acc = 0.535\ttrain_loss = 4.105e+00\n",
      "model #03, epoch #005:\ttrain_acc = 0.594\ttrain_loss = 4.049e+00\n",
      "model #03, epoch #006:\ttrain_acc = 0.615\ttrain_loss = 4.024e+00\n",
      "model #03, epoch #007:\ttrain_acc = 0.631\ttrain_loss = 4.005e+00\n",
      "model #03, epoch #008:\ttrain_acc = 0.642\ttrain_loss = 3.993e+00\n",
      "model #03, epoch #009:\ttrain_acc = 0.648\ttrain_loss = 3.985e+00\n"
     ]
    }
   ],
   "source": [
    "for k, model in enumerate(torch_models):\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    x = dataset.get_feature(split_name=f'train{k:03d}', feature_name='<default_input>')\n",
    "    y = dataset.get_feature(split_name=f'train{k:03d}', feature_name='<default_output>')\n",
    "    n_samples = x.shape[0]\n",
    "    n_batches = ceil(n_samples / batch_size)\n",
    "    x = np.array_split(x, n_batches)\n",
    "    y = np.array_split(y, n_batches)\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss, acc = 0.0, 0.0\n",
    "        for b in range(n_batches):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(Tensor(x[b]))\n",
    "            loss = criterion(Tensor(y[b]), y_pred)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            acc += torch.sum(y_pred.argmax(axis=1) == Tensor(y[b]).argmax(axis=1))\n",
    "        acc /= n_samples\n",
    "        epoch_loss /= n_samples\n",
    "        print(f'model #{k:02d}, epoch #{epoch:03d}:\\ttrain_acc = {acc:.3f}\\ttrain_loss = {epoch_loss:.3e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that the models are all train, we can wrap each of them in a `PytorchModel` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "models = [\n",
    "    PytorchModel(\n",
    "        model_obj=model,\n",
    "        loss_fn=criterion\n",
    "    )\n",
    "    for model in torch_models\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Information Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can now define two InformationSource objects. Basically, an information source is an abstraction representing a set of models, and their corresponding dataset. Note that we use the same `dataset` variable for the two InformationSource objects, but each will be queried on different splits of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "target_info_source = InformationSource(\n",
    "    models=[models[0]],\n",
    "    datasets=[dataset]\n",
    ")\n",
    "\n",
    "reference_info_source = InformationSource(\n",
    "    models=models[1:],\n",
    "    datasets=[dataset]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Metric and Audit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We now create a Metric object, which is an abstraction representing an algorithm used to measure something on an InformationSource, such as membership information leakage. In this case, we use the ShadowMetric to measure the membership information leakage of `target_info_source` in a black-box setting, using shadow models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "metric = ShadowMetric(\n",
    "    target_info_source=target_info_source,\n",
    "    reference_info_source=reference_info_source,\n",
    "    signals=[ModelLoss()],\n",
    "    hypothesis_test_func=threshold_func,\n",
    "    unique_dataset=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Finally, the Audit object is a wrapper to actually run the audit, and display the results. More visualization options will be added soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= METRIC RESULT OBJECT =============\n",
      "Accuracy          = 0.75\n",
      "ROC AUC Score     = 0.5\n",
      "FPR               = 1.0\n",
      "TN, FP, FN, TP    = (0, 12333, 0, 36999)\n"
     ]
    }
   ],
   "source": [
    "audit = Audit(\n",
    "    metric=metric,\n",
    "    target_info_source=target_info_source,\n",
    "    reference_info_source=reference_info_source\n",
    ")\n",
    "\n",
    "print(audit.run())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}